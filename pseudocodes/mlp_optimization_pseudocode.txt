ALGORITHM: Nature-Inspired Hyperparameter Optimization for MLP

INPUTS:
    Dataset X, y (Features and Labels)
    Search Space Limits:
        MIN_LAYERS = 1, MAX_LAYERS = 6
        UNITS = [16, 24, ..., 256]
        ACTIVATIONS = ["relu", "leaky_relu"]
        BATCH_SIZES = [256, 512, 1024, 2048]
    Optimizer Parameters:
        Algorithm (e.g., PSO)
        Population Size P
        Iterations T

OUTPUT:
    Best Trained Model M_star
    Test Metrics

BEGIN PROCESS:

    1. DEFINE OPTIMIZATION PROBLEM:
        Variable Bounds (Vector V):
            V[0]:     n_layers (Integer, [MIN_LAYERS, MAX_LAYERS])
            V[1]:     dropout_rate (Float, [0.0, 0.5])
            V[2]:     batch_norm (Boolean)
            For i in 0 to MAX_LAYERS-1:
                V[3 + 2*i]: units_layer_i (Categorical Index -> UNITS)
                V[4 + 2*i]: activation_layer_i (Categorical Index -> ACTIVATIONS)

    2. INITIALIZE OPTIMIZER (Mealpy):
        Create Swarm Optimizer (e.g., PSO) with Population P, Epochs T.

    3. START OPTIMIZATION LOOP (for each epoch t in 1..T):
        
        FOR each Agent A in Population:
            a. DECODE PARAMETERS (Vector V -> Hyperparameters H):
                n_layers <- Round(V[0])
                dropout <- Clip(V[1])
                batch_norm <- Bool(Round(V[2]))
                network_structure <- List()
                
                FOR k from 0 to n_layers-1:
                    units_k <- UNITS[V[3 + 2*k]]
                    act_k <- ACTIVATIONS[V[4 + 2*k]]
                    Add (units_k, act_k) to network_structure

            b. EVALUATE OBJECTIVE FUNCTION (Fitness):
                Split (X, y) into K-Fold CV (Stratified):
                Initialize F1_scores list
                
                FOR each fold (Train_Fold, Val_Fold):
                    i.  Build MLP Model M using H:
                        Input Layer -> [Dense(units) -> BatchNorm? -> Activation -> Dropout] x n_layers -> Output(Sigmoid)
                        Move M to DirectML Device (GPU)
                    
                    ii. Compute Class Weights (to handle imbalance)
                    
                    iii. Train M on Train_Fold:
                        Optimizer: Adam (lr=0.001)
                        Loss: Weighted BCEWithLogitsLoss
                        Loop for Max Epochs (with Early Stopping on Val_Fold)
                    
                    iv. Validate M on Val_Fold:
                        Compute F1 Score (Threshold=0.5)
                        Store F1 in F1_scores
                
                Objective Value = 1.0 - Mean(F1_scores)
                Return Objective Value

        UPDATE Population using Swarm Intelligence Rules (Vector updates)
        Log Global Best for Epoch t

    4. EXTRACT GLOBAL BEST SOLUTION:
        Best Vector V_star <- Optimizer.GlobalBest
        Best Hyperparameters H_star <- Decode(V_star)

    5. FINAL RETRAINING (Train on Train, Test on Test):
        Dataset: Full X_train, y_train
        Validation Set: Full X_test, y_test
        
        Build New Model M_final using H_star
        Move to GPU
        
        Train M_final on X_train (Full):
            Validation Metric: Loss on X_test
            Early Stopping: Monitor X_test Loss
            Save Best Checkpoint
            
        Load Best Checkpoint into M_final

    6. FINAL EVALUATION:
        Predict on X_test using M_final
        Compute Classification Report (Precision, Recall, F1, Accuracy)
        
    RETURN M_final, Metrics

END PROCESS
