{
 "cells": [
  {
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-11-16T17:41:19.673390Z",
     "start_time": "2025-11-16T17:41:13.889241Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import required packages\n",
    "import mealpy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, average_precision_score\n",
    "from globals.pandas_functions import *\n",
    "from mealpy.swarm_based import PSO\n",
    "from mealpy.swarm_based import GWO\n",
    "from mealpy.swarm_based import FA\n",
    "import globals.hyperparmeter_optimizer as hyp_optimizer\n",
    "import globals.model_evaluations as evaluations\n",
    "from mealpy.utils.space import IntegerVar, FloatVar, BoolVar, CategoricalVar\n",
    "import globals.gpu_processing as gpu_processing\n",
    "from tensorflow.keras import mixed_precision"
   ],
   "id": "9e03e967f20eb93e",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;31mAttributeError\u001B[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;31mAttributeError\u001B[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;31mAttributeError\u001B[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;31mAttributeError\u001B[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;31mAttributeError\u001B[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T17:41:22.069440Z",
     "start_time": "2025-11-16T17:41:20.287025Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# import datasets\n",
    "# load preprocessed training and testing data\n",
    "data_base_path = \"../data/processed/null_value_option_1/scaled_and_balanced\"\n",
    "\n",
    "X_train = pd.read_csv(f\"{data_base_path}/pca_selected_features/unified_transaction_data_option1_x_train_pca.csv\")\n",
    "X_test = pd.read_csv(f\"{data_base_path}/pca_selected_features/unified_transaction_data_option1_x_test_pca.csv\")\n",
    "y_train = pd.read_csv(f\"{data_base_path}/unified_transaction_data_option1_y_train_balanced.csv\")\n",
    "y_test = pd.read_csv(f\"{data_base_path}/unified_transaction_data_option1_y_test.csv\")"
   ],
   "id": "51ac4255da0d7f1a",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T17:41:25.425054Z",
     "start_time": "2025-11-16T17:41:25.409290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dataset_dimension(\"X_train\", X_train)\n",
    "dataset_dimension(\"X_test\", X_test)"
   ],
   "id": "51a0f96bcde2b3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train dataset dimension: (911764, 11)\n",
      "X_test dataset dimension: (118102, 11)\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T17:47:16.759836Z",
     "start_time": "2025-11-16T17:47:16.742139Z"
    }
   },
   "cell_type": "code",
   "source": "# above TF 2.10 no GPU native support",
   "id": "7289a17d85ccd4cc",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T18:00:26.892430Z",
     "start_time": "2025-11-16T18:00:26.879330Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# configure TF for GPU processing\n",
    "gpu_processing.configure_gpu(True, True, True)\n",
    "tf_strategy = gpu_processing.get_tf_strategy(False)\n",
    "tf_strategy = tf_strategy or tf.distribute.get_strategy()"
   ],
   "id": "165a6fbc3c21b780",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPUs: []\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## MLP Model Implementation with Hyperparameter Optimization",
   "id": "8b32e5a11d808f65"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T18:00:28.920652Z",
     "start_time": "2025-11-16T18:00:28.903010Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# define optimizer properties\n",
    "param_optimizer_algorithm = \"PSO\"   # \"PSO\", \"GWO\", \"FA\" (Firefly)\n",
    "population = 12     # number of candidate solutions\n",
    "iterations = 12     # optimization iterations (epochs in mealpy terms)\n",
    "cross_validation_folds = 3        # stratified folds for the objective\n",
    "epochs_for_evaluation = 12\n",
    "batch_size = 1024\n",
    "early_stopping = 3\n",
    "seed = 42"
   ],
   "id": "da47913770d4a0c7",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T18:00:31.713315Z",
     "start_time": "2025-11-16T18:00:31.700832Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# building ANN model\n",
    "def build_mlp_model(hyperparameters, input_shape,  lr=0.001):\n",
    "\n",
    "    with tf_strategy.scope():\n",
    "        try:\n",
    "            policy = mixed_precision.global_policy()\n",
    "            mixed_active = (policy and policy.name == 'mixed_float16')\n",
    "        except Exception:\n",
    "            mixed_active = False\n",
    "\n",
    "    # building the model\n",
    "\n",
    "    inputs = tf.keras.Input(shape=(input_shape,), name=\"inputs\")\n",
    "    x = inputs\n",
    "    for i, u in enumerate(hyperparameters[\"units_per_layer\"]):\n",
    "        x = tf.keras.layers.Dense(u, name=f\"dense_{i+1}\")(x)\n",
    "\n",
    "        if hyperparameters[\"batch_norm\"]:\n",
    "           x = tf.keras.layers.BatchNormalization(name=f\"bn_{i+1}\")(x)\n",
    "\n",
    "        x = hyp_optimizer.get_activation_layer(hyperparameters[\"activation\"])(x)\n",
    "\n",
    "        if hyperparameters[\"dropout_rate\"] > 0:\n",
    "            x = tf.keras.layers.Dropout(rate=hyperparameters[\"dropout_rate\"], name=f\"dropout_{i+1}\")(x)\n",
    "\n",
    "    out_dtype = \"float32\" if mixed_active else None  # IMPORTANT for mixed precision: keep the output in float32 for stable sigmoid/BCE\n",
    "    outputs = tf.keras.layers.Dense(1, name=\"probability\", activation=\"sigmoid\", dtype=out_dtype)(x)\n",
    "    mlp_model = tf.keras.Model(inputs, outputs, name=\"fraud_detection_mlp_model\")\n",
    "    mlp_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr), loss=\"binary_crossentropy\")\n",
    "\n",
    "    return mlp_model"
   ],
   "id": "405964d2636e2d0c",
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T18:00:33.691554Z",
     "start_time": "2025-11-16T18:00:33.679047Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def set_optimizer_objective(X, y, cv,  max_epochs, batch_size, seed, early_stopping_patience):\n",
    "    \"\"\"\n",
    "    Build the objective: minimize (1 - mean F1 across CV folds).\n",
    "    Simplicity choices:\n",
    "      - Fixed threshold=0.5 for F1\n",
    "      - EarlyStopping on val_loss\n",
    "      - No SMOTE here (assume your training set is already prepared)\n",
    "    \"\"\"\n",
    "    splits = list(StratifiedKFold(n_splits=cv, shuffle=True, random_state=seed).split(np.zeros_like(y), y))\n",
    "\n",
    "    def obj(vec):\n",
    "        hp = hyp_optimizer.optimizer_vectors_to_mlp_hyperparams( vec)\n",
    "        f1_scores = []\n",
    "        for tr_idx, va_idx in splits:\n",
    "            X_tr, y_tr = X[tr_idx], y[tr_idx]\n",
    "            X_va, y_va = X[va_idx], y[va_idx]\n",
    "\n",
    "            mlp_model = build_mlp_model(hp, X.shape[1], 1e-3)  # LR 0.001 fixed for simplicity\n",
    "            es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True, verbose=0)\n",
    "            mlp_model.fit(X_tr, y_tr, validation_data=(X_va, y_va),\n",
    "                      epochs=max_epochs, batch_size=batch_size, verbose=0, callbacks=[es])\n",
    "\n",
    "            y_prob = mlp_model.predict(X_va, verbose=0).ravel()\n",
    "            f1_scores.append(evaluations.classification_metrics(y_va, y_prob, threshold=0.5)[\"f1\"])\n",
    "\n",
    "        # minimize 1 - mean(F1)\n",
    "        return 1.0 - float(np.mean(f1_scores))\n",
    "\n",
    "    return obj"
   ],
   "id": "fb6e7c67571e6ba",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T18:00:35.758868Z",
     "start_time": "2025-11-16T18:00:35.734031Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# hyperparameter optimizer runner\n",
    "def run_hyperparam_optimization(algorithm,\n",
    "                  objective,\n",
    "                  iterations=iterations,\n",
    "                  population=population):\n",
    "    \"\"\"\n",
    "    Run the selected mealpy optimizer and print per-epoch global best parameters.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    algorithm : str\n",
    "        One of {'PSO', 'GWO', 'FA'}.\n",
    "    objective : callable\n",
    "        Function f(vec) -> scalar (1 - mean F1). Created by make_objective().\n",
    "    iterations : int\n",
    "        Number of optimization epochs (mealpy 'epoch').\n",
    "    population : int\n",
    "        Population size (mealpy 'pop_size').\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    best_vec : array-like\n",
    "        Final best solution vector (continuous).\n",
    "    best_obj : float\n",
    "        Final best objective value (1 - mean F1).\n",
    "    best_hp : dict\n",
    "        Decoded hyperparameters from best_vec.\n",
    "    epoch_logs : list[dict]\n",
    "        Per-epoch records with keys:\n",
    "          'epoch', 'f1', 'objective', 'vector',\n",
    "          'n_layers', 'units', 'activation', 'batch_norm', 'dropout'\n",
    "    \"\"\"\n",
    "    # Bounds match vec_to_hp layout:\n",
    "    # [n_layers, units1, units2, units3, units4, dropout, activation_code, batch_norm_flag, (unused), (unused)]\n",
    "    lower_bounds = [1, 16, 16, 16, 16, 0.0, 0, 0, 0.0, 0.0]\n",
    "    upper_bounds = [4, 256, 256, 256, 256, 0.5, 2, 1, 1.0, 1.0]\n",
    "\n",
    "    # problem = dict(fit_func=objective, lb=lower_bounds, ub=upper_bounds, minmax=\"min\", log_to=None)\n",
    "\n",
    "    # problem_lbub = dict(fit_func=objective, lb=lower_bounds, ub=upper_bounds, minmax=\"min\", log_to=None)\n",
    "    # problem_bounds = dict(obj_func=objective, bounds=list(zip(lower_bounds, upper_bounds)), minmax=\"min\", log_to=None)\n",
    "\n",
    "    # Multiples of 8 from 16 to 256 inclusive\n",
    "\n",
    "\n",
    "    optimizer_bounds = [\n",
    "        IntegerVar(lb=1, ub=4),               # n_layers\n",
    "        CategoricalVar(hyp_optimizer.hidden_layer_unit_choices),         # units1\n",
    "        CategoricalVar(hyp_optimizer.hidden_layer_unit_choices),         # units2\n",
    "        CategoricalVar(hyp_optimizer.hidden_layer_unit_choices),         # units3\n",
    "        CategoricalVar(hyp_optimizer.hidden_layer_unit_choices),         # units4\n",
    "        FloatVar(lb=0.0, ub=0.5),             # dropout\n",
    "        CategoricalVar(hyp_optimizer.activation_functions),          # activation\n",
    "        BoolVar(),                            # batch_norm\n",
    "    ]\n",
    "\n",
    "    problem = dict(obj_func=objective, bounds=optimizer_bounds, minmax=\"min\", log_to=None)\n",
    "\n",
    "\n",
    "    # Select optimizer\n",
    "    if algorithm.upper() == \"PSO\":\n",
    "        optimizer = PSO.OriginalPSO(epoch=iterations, pop_size=population)\n",
    "    elif algorithm.upper() == \"GWO\":\n",
    "        optimizer = GWO.OriginalGWO(epoch=iterations, pop_size=population)\n",
    "    elif algorithm.upper() == \"FA\":\n",
    "        optimizer = FA.OriginalFA(epoch=iterations, pop_size=population)\n",
    "    else:\n",
    "        raise ValueError(\"algorithm must be one of {'PSO','GWO','FA'}\")\n",
    "\n",
    "    # Run optimization\n",
    "    best_vec, best_obj = optimizer.solve(problem)\n",
    "    best_hp = hyp_optimizer.optimizer_vectors_to_mlp_hyperparams(best_vec)\n",
    "\n",
    "    # Collect per-epoch history (mealpy stores global best progression)\n",
    "    epoch_logs = []\n",
    "    try:\n",
    "        history_vectors = getattr(optimizer.history, \"list_global_best\", [])\n",
    "        history_scores = getattr(optimizer.history, \"list_global_best_fitness\", [])\n",
    "    except AttributeError:\n",
    "        history_vectors, history_scores = [], []\n",
    "\n",
    "    total_epochs = len(history_scores)\n",
    "\n",
    "    print(\"\\n=== Per-Epoch Global Best Log ===\")\n",
    "\n",
    "    for i in range(total_epochs):\n",
    "        vec_epoch = history_vectors[i]\n",
    "        obj_epoch = history_scores[i]\n",
    "        f1_epoch = 1.0 - obj_epoch  # Because objective = 1 - F1\n",
    "        hp_epoch = hyp_optimizer.optimizer_vectors_to_mlp_hyperparams(vec_epoch)\n",
    "        log_record = {\n",
    "            \"epoch\": i + 1,\n",
    "            \"f1\": float(f1_epoch),\n",
    "            \"objective\": float(obj_epoch),\n",
    "            \"vector\": [float(v) for v in vec_epoch],\n",
    "            \"n_layers\": hp_epoch[\"n_layers\"],\n",
    "            \"units\": hp_epoch[\"units\"],\n",
    "            \"activation\": hp_epoch[\"activation\"],\n",
    "            \"batch_norm\": hp_epoch[\"batch_norm\"],\n",
    "            \"dropout\": hp_epoch[\"dropout\"],\n",
    "        }\n",
    "        epoch_logs.append(log_record)\n",
    "\n",
    "        print(f\"Epoch {i+1:02d}/{total_epochs} | F1={f1_epoch:.4f} | Layers={hp_epoch['n_layers']} \"\n",
    "              f\"| Units={hp_epoch['units']} | Act={hp_epoch['activation']} \"\n",
    "              f\"| BN={hp_epoch['batch_norm']} | Dropout={hp_epoch['dropout']:.3f}\")\n",
    "\n",
    "    print(\"\\n=== Final Best Solution ===\")\n",
    "    print(f\"Best objective (1 - F1): {best_obj:.6f}  => F1={1 - best_obj:.6f}\")\n",
    "    print(\"Best hyperparameters:\", best_hp)\n",
    "\n",
    "    return best_vec, best_obj, best_hp, epoch_logs"
   ],
   "id": "fb402d6941360f9f",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-16T18:00:39.853276Z",
     "start_time": "2025-11-16T18:00:39.846279Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def retrain_and_evaluate(best_hp, X_train, y_train, X_test, y_test,\n",
    "                         max_epochs=40, batch=batch_size, early_stopping_patience=8):\n",
    "    \"\"\"\n",
    "    Retrain the best architecture on all training data (with val_split=0.1),\n",
    "    then evaluate on the untouched test set at threshold=0.5.\n",
    "    \"\"\"\n",
    "    mlp_model = build_mlp_model(best_hp, X_train.shape[1], 1e-3)\n",
    "    es = tf.keras.callbacks.EarlyStopping(monitor=\"val_loss\", patience=early_stopping_patience, restore_best_weights=True)\n",
    "    mlp_model.fit(X_train, y_train, validation_split=0.1, epochs=max_epochs, batch_size=batch, verbose=1, callbacks=[es])\n",
    "\n",
    "    y_prob = mlp_model.predict(X_test, verbose=0).ravel()\n",
    "    metrics = evaluations.classification_metrics(y_test, y_prob, threshold=0.5)\n",
    "    return mlp_model, metrics"
   ],
   "id": "c0fe0b2c79f8a7e6",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-11-16T18:00:41.499215Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# execute hyperparameter optimization\n",
    "optimizer_algorithm = \"PSO\"   # \"PSO\", \"GWO\", \"FA\" (Firefly)\n",
    "optimizer_objective = set_optimizer_objective(X_train.to_numpy(),\n",
    "                                              y_train.to_numpy().ravel(),\n",
    "                                              cv=cross_validation_folds,\n",
    "                                              max_epochs=epochs_for_evaluation,\n",
    "                                              batch_size=batch_size,\n",
    "                                              seed=seed,\n",
    "                                              early_stopping_patience=early_stopping)\n",
    "\n",
    "print(f\"\\nRunning optimizer: {optimizer_algorithm} (pop={population}, iters={iterations})\")\n",
    "\n",
    "best_vector, best_objective, best_hyper_params = run_hyperparam_optimization(optimizer_algorithm, optimizer_objective, iterations=iterations, population=population)\n",
    "print(\"\\nBest objective (1 - F1):\", best_objective)\n",
    "print(\"Best hyperparameters:\", best_hyper_params)\n"
   ],
   "id": "6e2fc0eb4d186b03",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running optimizer: PSO (pop=12, iters=12)\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\nRetraining best model on full training set and evaluating on test...\")\n",
    "model, test_metrics = retrain_and_evaluate(best_hyper_params, X_train.to_numpy(), y_train.to_numpy().ravel(), X_test.to_numpy(), y_test.to_numpy().ravel())\n",
    "print(\"\\nTest metrics @ threshold=0.5\")\n",
    "for k, v in test_metrics.items():\n",
    "    print(f\"  {k}: {v:.4f}\")"
   ],
   "id": "5519e4abd5444a75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "# TODO: export the best model",
   "id": "7e3b2c6a27e8ea27"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
